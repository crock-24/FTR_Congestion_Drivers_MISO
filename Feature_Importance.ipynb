{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ae4e123-bec1-4ce0-b339-197e708e4e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#read in data needed \n",
    "#from https://www.misoenergy.org/markets-and-operations/real-time--market-data/market-reports/#nt=%2FMarketReportType%3AHistorical%20LMP%2FMarketReportName%3ADay-Ahead%20EPNode%20LMP%20(zip)&t=10&p=0&s=MarketReportPublished&sd=desc\n",
    "DA_LMP = pd.read_csv('data/cleaned/DA/miso_da_combined_clean.csv', index_col = 0)\n",
    "\n",
    "#converting datetime to datetime on loadtempwind so it can be merged with LMP data\n",
    "DA_LMP['datetime'] = pd.to_datetime(DA_LMP['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13fb395d-de09-4028-b015-ee499e9273ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.eia.gov/electricity/wholesalemarkets/data.php?rto=miso\n",
    "Load_Temp_Wind = pd.read_csv('data/cleaned/Features/miso_load-temp-wind_hr_2025.csv',  index_col = 0)\n",
    "\n",
    "#converting datetime to datetime on loadtempwind so it can be merged with LMP data\n",
    "Load_Temp_Wind['datetime'] = pd.to_datetime(Load_Temp_Wind['datetime'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07b80ad1-4034-45c1-8581-0e87bcd1df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will only keep rows where dates match in both DataFrames\n",
    "DA_LMP = pd.merge(DA_LMP, Load_Temp_Wind, on='datetime', how='outer')\n",
    "\n",
    "# For any remaining NAs use forward and backward fill\n",
    "DA_LMP = DA_LMP.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2636c9c-4d12-4eec-aa13-c3ca4f7f800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in day ahead constraint data\n",
    "constraint_pivot = pd.read_csv('data/cleaned/DA_Constraints/combined_DA_constraints.csv')\n",
    "\n",
    "# Preprocess constraint data \n",
    "constraint_pivot['datetime'] = pd.to_datetime(constraint_pivot['datetime'])\n",
    "constraint_pivot = constraint_pivot.set_index('datetime')\n",
    "\n",
    "# Filter for constraints with sufficient data (24+ hours)\n",
    "min_obs = 24\n",
    "valid_constraints = constraint_pivot.columns[constraint_pivot.notna().sum() > min_obs]\n",
    "constraint_pivot = constraint_pivot[valid_constraints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "842ebd0d-4dde-496e-a49c-5bdfdaffea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lagged correlation with temp\n",
    "def calc_corr_lagged_temp(group, lag=1):\n",
    "    # Shift wind by 1 hour (lag), so wind[t-1] lines up with congestion[t]\n",
    "    lagged_temp = group['Temperature'].shift(lag)\n",
    "    \n",
    "    # Drop rows where lag introduces NaN to avoid invalid correlation\n",
    "    valid = group[['congestion']].copy()\n",
    "    valid['lagged_temp'] = lagged_temp\n",
    "    \n",
    "    valid = valid.dropna()\n",
    "\n",
    "    return valid['congestion'].corr(valid['lagged_temp'])\n",
    "    \n",
    "# Lagged correlation with load\n",
    "def calc_corr_lagged_load(group, lag=1):\n",
    "    # Shift wind by 1 hour (lag), so wind[t-1] lines up with congestion[t]\n",
    "    lagged_load = group['Total Forecast Load'].shift(lag)\n",
    "    \n",
    "    # Drop rows where lag introduces NaN to avoid invalid correlation\n",
    "    valid = group[['congestion']].copy()\n",
    "    valid['lagged_load'] = lagged_load\n",
    "    \n",
    "    valid = valid.dropna()\n",
    "\n",
    "    return valid['congestion'].corr(valid['lagged_load'])\n",
    "    \n",
    "# Lagged correlation with wind\n",
    "def calc_corr_lagged_wind(group, lag=1):\n",
    "    # Shift wind by 1 hour (lag), so wind[t-1] lines up with congestion[t]\n",
    "    lagged_wind = group['Total Wind Gen'].shift(lag)\n",
    "    \n",
    "    # Drop rows where lag introduces NaN to avoid invalid correlation\n",
    "    valid = group[['congestion']].copy()\n",
    "    valid['lagged_wind'] = lagged_wind\n",
    "    \n",
    "    valid = valid.dropna()\n",
    "\n",
    "    return valid['congestion'].corr(valid['lagged_wind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85acfed6-ea61-4807-9cf4-ce6078014224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# congestion correlation temp\n",
    "temp_corr_by_node_lagged_18 = DA_LMP.groupby('node').apply(calc_corr_lagged_temp, lag = 18)\n",
    "\n",
    "# congestion correlation temp\n",
    "temp_corr_by_node_lagged_12 = DA_LMP.groupby('node').apply(calc_corr_lagged_temp, lag = 12)\n",
    "\n",
    "# congestion correlation temp\n",
    "temp_corr_by_node_lagged_6 = DA_LMP.groupby('node').apply(calc_corr_lagged_temp, lag = 6)\n",
    "\n",
    "# congestion correlation temp\n",
    "temp_corr_by_node_lagged_2 = DA_LMP.groupby('node').apply(calc_corr_lagged_temp, lag = 2)\n",
    "\n",
    "# congestion correlation temp\n",
    "temp_corr_by_node = DA_LMP.groupby('node').apply(calc_corr_lagged_temp, lag = 0)\n",
    "\n",
    "# congestion correlation load\n",
    "load_corr_by_node_lagged_18 = DA_LMP.groupby('node').apply(calc_corr_lagged_load, lag = 18)\n",
    "\n",
    "# congestion correlation load\n",
    "load_corr_by_node_lagged_12 = DA_LMP.groupby('node').apply(calc_corr_lagged_load, lag = 12)\n",
    "\n",
    "# congestion correlation load\n",
    "load_corr_by_node_lagged_6 = DA_LMP.groupby('node').apply(calc_corr_lagged_load, lag = 6)\n",
    "\n",
    "# congestion correlation load\n",
    "load_corr_by_node_lagged_2 = DA_LMP.groupby('node').apply(calc_corr_lagged_load, lag = 2)\n",
    "\n",
    "# congestion correlation load\n",
    "load_corr_by_node = DA_LMP.groupby('node').apply(calc_corr_lagged_load, lag = 0)\n",
    "\n",
    "# congestion correlation wind\n",
    "wind_corr_by_node_lagged_18 = DA_LMP.groupby('node').apply(calc_corr_lagged_wind, lag = 18)\n",
    "\n",
    "# congestion correlation wind\n",
    "wind_corr_by_node_lagged_12 = DA_LMP.groupby('node').apply(calc_corr_lagged_wind, lag = 12)\n",
    "\n",
    "# congestion correlation wind\n",
    "wind_corr_by_node_lagged_6 = DA_LMP.groupby('node').apply(calc_corr_lagged_wind, lag = 6)\n",
    "\n",
    "# congestion correlation wind\n",
    "wind_corr_by_node_lagged_2 = DA_LMP.groupby('node').apply(calc_corr_lagged_wind, lag = 2)\n",
    "\n",
    "# congestion correlation wind\n",
    "wind_corr_by_node = DA_LMP.groupby('node').apply(calc_corr_lagged_wind, lag = 0)\n",
    "\n",
    "#combining metrics together in one dataframe\n",
    "CorrCombined = pd.concat([temp_corr_by_node_lagged_18, temp_corr_by_node_lagged_12, temp_corr_by_node_lagged_6, temp_corr_by_node_lagged_2, temp_corr_by_node, load_corr_by_node_lagged_18, load_corr_by_node_lagged_12, load_corr_by_node_lagged_6, load_corr_by_node_lagged_2, load_corr_by_node, wind_corr_by_node_lagged_18, wind_corr_by_node_lagged_12, wind_corr_by_node_lagged_6, wind_corr_by_node_lagged_2, wind_corr_by_node], axis = 1, ignore_index=False)\n",
    "\n",
    "#Renaming columns for clarity\n",
    "CorrCombined.columns = ['temp correlation lagged 18 hours', 'temp correlation lagged 12 hours', 'temp correlation lagged 6 hours', 'temp correlation lagged 2 hours', 'temp correlation 0 lag', 'load correlation lagged 18 hours', 'load correlation lagged 12 hours', 'load correlation lagged 6 hours', 'load correlation lagged 2 hours', 'load correlation 0 lag', 'wind correlation lagged 18 hours', 'wind correlation lagged 12 hours', 'wind correlation lagged 6 hours', 'wind correlation lagged 2 hours', 'wind correlation 0 lag']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e9d9be6-62a0-4ca3-a950-490d9801b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot LMP data to wide format: datetime x node\n",
    "DA_LMP['datetime'] = pd.to_datetime(DA_LMP['datetime'])\n",
    "lmp_wide = DA_LMP.pivot(index='datetime', columns='node', values='congestion')\n",
    "\n",
    "# Join once on datetime index\n",
    "combined = constraint_pivot.join(lmp_wide, how='inner')\n",
    "\n",
    "# Compute correlation matrix (constraints x nodes) \n",
    "constraint_cols = constraint_pivot.columns\n",
    "node_cols = lmp_wide.columns\n",
    "\n",
    "# Create an empty DataFrame to store correlations\n",
    "correlation_df = pd.DataFrame(index=constraint_cols, columns=node_cols)\n",
    "\n",
    "# Calculate correlations between each constraint and each node\n",
    "for constraint in constraint_cols:\n",
    "    for node in node_cols:\n",
    "        correlation_df.loc[constraint, node] = combined[constraint].corr(combined[node])\n",
    "\n",
    "# Get top 5 correlated constraints for each node \n",
    "top_corr_list = []\n",
    "for node in correlation_df.columns:\n",
    "    node_corr = correlation_df[node].dropna()\n",
    "    top5 = node_corr.abs().sort_values(ascending=False).head(5)\n",
    "    actual_corrs = node_corr.loc[top5.index]\n",
    "    \n",
    "    result_row = {'node': node}\n",
    "    for i, (constraint, corr_val) in enumerate(actual_corrs.items(), 1):\n",
    "        result_row[f'constraint_{i}'] = constraint\n",
    "        result_row[f'constraint_{i}_corr'] = corr_val\n",
    "\n",
    "    top_corr_list.append(result_row)\n",
    "\n",
    "# Final output\n",
    "top_corr_df = pd.DataFrame(top_corr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d25166a-f31d-4191-bfee-f3bda8c6715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting index for top constraint correlation\n",
    "top_corr_df = top_corr_df.set_index('node')\n",
    "\n",
    "#merging top constraint based congestion drivers with top feature congestion drivers\n",
    "CorrCombined = pd.merge(top_corr_df, CorrCombined, on=CorrCombined.index, how='inner')\n",
    "\n",
    "# Rename for consistency\n",
    "CorrCombined = CorrCombined.rename(columns={'key_0': 'LMP node'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c77db1ab-4e8b-48b0-9289-3b90fbaadd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save node correlations to a .csv file\n",
    "CorrCombined.to_csv('Outputs/Top_LMP_Node_Correlations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f48b5ff-8a44-4502-b3c3-5a94622f67f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a62773-3c87-4283-a1c8-fed2b7a0bcb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4a547-18c1-447a-9263-daa706aba377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
