{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64f74cd5-484f-48bb-afbc-d4a7bca2d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd5a5402-7890-4f5c-8ad1-7888fe0ebff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processes a single wide-format MISO Day-Ahead EPNode CSV and returns a long-format DataFrame with datetime, node, and LMP components.\n",
    "#Data Source: https://www.misoenergy.org/markets-and-operations/real-time--market-data/market-reports/#nt=%2FMarketReportType%3AHistorical%20LMP%2FMarketReportName%3ADay-Ahead%20EPNode%20LMP%20(zip)&t=10&p=3&s=MarketReportPublished&sd=desc\n",
    "def process_wide_format_safely(file_path):\n",
    "\n",
    "    # Step 1: Read lines and find header row\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Locate header row (should contain \"EPNode\" and \"HE1\")\n",
    "    header_row_index = next(\n",
    "        (i for i, line in enumerate(lines) if \"EPNode\" in line and \"HE1\" in line),\n",
    "        None\n",
    "    )\n",
    "\n",
    "    if header_row_index is None:\n",
    "        raise ValueError(\"Could not find valid header row in file.\")\n",
    "\n",
    "    # Step 2: Extract market date (e.g., \"Market Day: 06/05/2025\")\n",
    "    market_day_line = next(\n",
    "        (line for line in lines if \"Market Day\" in line),\n",
    "        None\n",
    "    )\n",
    "    if not market_day_line:\n",
    "        raise ValueError(\"Market Day not found in file.\")\n",
    "\n",
    "    market_day_str = market_day_line.split(\":\")[-1].strip()\n",
    "    market_date = datetime.strptime(market_day_str, \"%m/%d/%Y\")\n",
    "\n",
    "    # Step 3: Load CSV from the correct header row\n",
    "    df = pd.read_csv(file_path, skiprows=header_row_index)\n",
    "\n",
    "    # Step 4: Convert wide to long format\n",
    "    long_data = []\n",
    "\n",
    "    for i in range(0, len(df) - 2, 3):  # Process 3 rows at a time (LMP, MCC, MLC)\n",
    "        try:\n",
    "            row_lmp = df.iloc[i]\n",
    "            row_mcc = df.iloc[i + 1]\n",
    "            row_mlc = df.iloc[i + 2]\n",
    "\n",
    "            # Safe access to first column (node name), drop leading \"L\"\n",
    "            node = \" \".join(str(row_lmp.iloc[0]).split()[1:])\n",
    "\n",
    "            for hour in range(1, 25):  # HE1 to HE24\n",
    "                hour_col = f\"HE{hour}\"\n",
    "                dt = market_date + timedelta(hours=hour - 1)\n",
    "\n",
    "                long_data.append({\n",
    "                    \"datetime\": dt,\n",
    "                    \"node\": node,\n",
    "                    \"lmp_total\": float(row_lmp[hour_col]),\n",
    "                    \"congestion\": float(row_mcc[hour_col]),\n",
    "                    \"loss\": float(row_mlc[hour_col])\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing rows {i}-{i+2}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(long_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36568fbf-dc12-47e4-a24a-edcfc24148e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file data/cleaned/DA/miso_da_combined_clean.csv does not exist. Running the processing code...\n",
      "Processing DA_Load_EPNodes_20250401.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250401_clean.csv\n",
      "Processing DA_Load_EPNodes_20250402.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250402_clean.csv\n",
      "Processing DA_Load_EPNodes_20250403.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250403_clean.csv\n",
      "Processing DA_Load_EPNodes_20250404.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250404_clean.csv\n",
      "Processing DA_Load_EPNodes_20250405.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250405_clean.csv\n",
      "Processing DA_Load_EPNodes_20250406.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250406_clean.csv\n",
      "Processing DA_Load_EPNodes_20250407.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250407_clean.csv\n",
      "Processing DA_Load_EPNodes_20250408.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250408_clean.csv\n",
      "Processing DA_Load_EPNodes_20250409.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250409_clean.csv\n",
      "Processing DA_Load_EPNodes_20250410.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250410_clean.csv\n",
      "Processing DA_Load_EPNodes_20250411.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250411_clean.csv\n",
      "Processing DA_Load_EPNodes_20250412.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250412_clean.csv\n",
      "Processing DA_Load_EPNodes_20250413.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250413_clean.csv\n",
      "Processing DA_Load_EPNodes_20250414.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250414_clean.csv\n",
      "Processing DA_Load_EPNodes_20250415.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250415_clean.csv\n",
      "Processing DA_Load_EPNodes_20250416.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250416_clean.csv\n",
      "Processing DA_Load_EPNodes_20250417.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250417_clean.csv\n",
      "Processing DA_Load_EPNodes_20250418.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250418_clean.csv\n",
      "Processing DA_Load_EPNodes_20250419.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250419_clean.csv\n",
      "Processing DA_Load_EPNodes_20250420.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250420_clean.csv\n",
      "Processing DA_Load_EPNodes_20250421.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250421_clean.csv\n",
      "Processing DA_Load_EPNodes_20250422.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250422_clean.csv\n",
      "Processing DA_Load_EPNodes_20250423.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250423_clean.csv\n",
      "Processing DA_Load_EPNodes_20250424.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250424_clean.csv\n",
      "Processing DA_Load_EPNodes_20250425.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250425_clean.csv\n",
      "Processing DA_Load_EPNodes_20250426.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250426_clean.csv\n",
      "Processing DA_Load_EPNodes_20250427.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250427_clean.csv\n",
      "Processing DA_Load_EPNodes_20250428.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250428_clean.csv\n",
      "Processing DA_Load_EPNodes_20250429.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250429_clean.csv\n",
      "Processing DA_Load_EPNodes_20250430.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250430_clean.csv\n",
      "Processing DA_Load_EPNodes_20250501.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250501_clean.csv\n",
      "Processing DA_Load_EPNodes_20250502.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250502_clean.csv\n",
      "Processing DA_Load_EPNodes_20250503.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250503_clean.csv\n",
      "Processing DA_Load_EPNodes_20250504.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250504_clean.csv\n",
      "Processing DA_Load_EPNodes_20250505.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250505_clean.csv\n",
      "Processing DA_Load_EPNodes_20250506.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250506_clean.csv\n",
      "Processing DA_Load_EPNodes_20250507.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250507_clean.csv\n",
      "Processing DA_Load_EPNodes_20250508.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250508_clean.csv\n",
      "Processing DA_Load_EPNodes_20250509.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250509_clean.csv\n",
      "Processing DA_Load_EPNodes_20250510.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250510_clean.csv\n",
      "Processing DA_Load_EPNodes_20250511.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250511_clean.csv\n",
      "Processing DA_Load_EPNodes_20250512.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250512_clean.csv\n",
      "Processing DA_Load_EPNodes_20250513.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250513_clean.csv\n",
      "Processing DA_Load_EPNodes_20250514.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250514_clean.csv\n",
      "Processing DA_Load_EPNodes_20250515.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250515_clean.csv\n",
      "Processing DA_Load_EPNodes_20250516.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250516_clean.csv\n",
      "Processing DA_Load_EPNodes_20250517.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250517_clean.csv\n",
      "Processing DA_Load_EPNodes_20250518.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250518_clean.csv\n",
      "Processing DA_Load_EPNodes_20250519.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250519_clean.csv\n",
      "Processing DA_Load_EPNodes_20250520.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250520_clean.csv\n",
      "Processing DA_Load_EPNodes_20250521.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250521_clean.csv\n",
      "Processing DA_Load_EPNodes_20250522.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250522_clean.csv\n",
      "Processing DA_Load_EPNodes_20250523.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250523_clean.csv\n",
      "Processing DA_Load_EPNodes_20250524.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250524_clean.csv\n",
      "Processing DA_Load_EPNodes_20250525.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250525_clean.csv\n",
      "Processing DA_Load_EPNodes_20250526.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250526_clean.csv\n",
      "Processing DA_Load_EPNodes_20250527.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250527_clean.csv\n",
      "Processing DA_Load_EPNodes_20250528.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250528_clean.csv\n",
      "Processing DA_Load_EPNodes_20250529.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250529_clean.csv\n",
      "Processing DA_Load_EPNodes_20250530.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250530_clean.csv\n",
      "Processing DA_Load_EPNodes_20250531.csv ...\n",
      "Saved cleaned file to data/dirty/DA/Long/miso_da_20250531_clean.csv\n"
     ]
    }
   ],
   "source": [
    "dirty_folder = \"data/dirty/DA/Wide\"\n",
    "cleaned_folder = \"data/dirty/DA/Long\"\n",
    "\n",
    "os.makedirs(cleaned_folder, exist_ok=True)  # make sure cleaned folder exists\n",
    "file_path = 'data/cleaned/DA/miso_da_combined_clean.csv'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"The file {file_path} does not exist. Running the processing code...\")\n",
    "    for filename in sorted(os.listdir(dirty_folder)):\n",
    "        if filename.startswith(\"DA_Load_EPNodes\") and filename.endswith(\".csv\"):\n",
    "            input_path = os.path.join(dirty_folder, filename)\n",
    "            output_path = os.path.join(cleaned_folder, filename.replace(\"DA_Load_EPNodes\", \"miso_da\")[:-4] + \"_clean.csv\")\n",
    "            \n",
    "            print(f\"Processing {filename} ...\")\n",
    "            try:\n",
    "                df = process_wide_format_safely(input_path)\n",
    "                df.to_csv(output_path, index=False)\n",
    "                print(f\"Saved cleaned file to {output_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {filename}: {e}\")\n",
    "else:\n",
    "    print(f\"The file {file_path} already exists. Skipping processing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66b013a4-19d8-4184-a146-c726a8892332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_cleaned_csvs(folder=\"data/dirty/DA/Long\", prefix=\"miso_da_\", ext=\".csv\"):\n",
    "    all_dfs = []\n",
    "    \n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        if filename.startswith(prefix) and filename.endswith(ext):\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            print(f\"📄 Loading: {filename}\")\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, parse_dates=[\"datetime\"])\n",
    "                all_dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to load {filename}: {e}\")\n",
    "\n",
    "    if not all_dfs:\n",
    "        print(\"❌ No files combined.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    combined_df.sort_values(by=[\"datetime\", \"node\"], inplace=True)\n",
    "    combined_df.to_csv(\"data/cleaned/DA/miso_da_combined_clean.csv\")\n",
    "    print(f\"✅ Combined dataframe saved\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d130e2e4-940b-4985-87b9-08fe13356d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file data/cleaned/DA/miso_da_combined_clean.csv does not exist. Running the processing code...\n",
      "📄 Loading: miso_da_20250401_clean.csv\n",
      "📄 Loading: miso_da_20250402_clean.csv\n",
      "📄 Loading: miso_da_20250403_clean.csv\n",
      "📄 Loading: miso_da_20250404_clean.csv\n",
      "📄 Loading: miso_da_20250405_clean.csv\n",
      "📄 Loading: miso_da_20250406_clean.csv\n",
      "📄 Loading: miso_da_20250407_clean.csv\n",
      "📄 Loading: miso_da_20250408_clean.csv\n",
      "📄 Loading: miso_da_20250409_clean.csv\n",
      "📄 Loading: miso_da_20250410_clean.csv\n",
      "📄 Loading: miso_da_20250411_clean.csv\n",
      "📄 Loading: miso_da_20250412_clean.csv\n",
      "📄 Loading: miso_da_20250413_clean.csv\n",
      "📄 Loading: miso_da_20250414_clean.csv\n",
      "📄 Loading: miso_da_20250415_clean.csv\n",
      "📄 Loading: miso_da_20250416_clean.csv\n",
      "📄 Loading: miso_da_20250417_clean.csv\n",
      "📄 Loading: miso_da_20250418_clean.csv\n",
      "📄 Loading: miso_da_20250419_clean.csv\n",
      "📄 Loading: miso_da_20250420_clean.csv\n",
      "📄 Loading: miso_da_20250421_clean.csv\n",
      "📄 Loading: miso_da_20250422_clean.csv\n",
      "📄 Loading: miso_da_20250423_clean.csv\n",
      "📄 Loading: miso_da_20250424_clean.csv\n",
      "📄 Loading: miso_da_20250425_clean.csv\n",
      "📄 Loading: miso_da_20250426_clean.csv\n",
      "📄 Loading: miso_da_20250427_clean.csv\n",
      "📄 Loading: miso_da_20250428_clean.csv\n",
      "📄 Loading: miso_da_20250429_clean.csv\n",
      "📄 Loading: miso_da_20250430_clean.csv\n",
      "📄 Loading: miso_da_20250501_clean.csv\n",
      "📄 Loading: miso_da_20250502_clean.csv\n",
      "📄 Loading: miso_da_20250503_clean.csv\n",
      "📄 Loading: miso_da_20250504_clean.csv\n",
      "📄 Loading: miso_da_20250505_clean.csv\n",
      "📄 Loading: miso_da_20250506_clean.csv\n",
      "📄 Loading: miso_da_20250507_clean.csv\n",
      "📄 Loading: miso_da_20250508_clean.csv\n",
      "📄 Loading: miso_da_20250509_clean.csv\n",
      "📄 Loading: miso_da_20250510_clean.csv\n",
      "📄 Loading: miso_da_20250511_clean.csv\n",
      "📄 Loading: miso_da_20250512_clean.csv\n",
      "📄 Loading: miso_da_20250513_clean.csv\n",
      "📄 Loading: miso_da_20250514_clean.csv\n",
      "📄 Loading: miso_da_20250515_clean.csv\n",
      "📄 Loading: miso_da_20250516_clean.csv\n",
      "📄 Loading: miso_da_20250517_clean.csv\n",
      "📄 Loading: miso_da_20250518_clean.csv\n",
      "📄 Loading: miso_da_20250519_clean.csv\n",
      "📄 Loading: miso_da_20250520_clean.csv\n",
      "📄 Loading: miso_da_20250521_clean.csv\n",
      "📄 Loading: miso_da_20250522_clean.csv\n",
      "📄 Loading: miso_da_20250523_clean.csv\n",
      "📄 Loading: miso_da_20250524_clean.csv\n",
      "📄 Loading: miso_da_20250525_clean.csv\n",
      "📄 Loading: miso_da_20250526_clean.csv\n",
      "📄 Loading: miso_da_20250527_clean.csv\n",
      "📄 Loading: miso_da_20250528_clean.csv\n",
      "📄 Loading: miso_da_20250529_clean.csv\n",
      "📄 Loading: miso_da_20250530_clean.csv\n",
      "📄 Loading: miso_da_20250531_clean.csv\n",
      "✅ Combined dataframe saved\n",
      "Processing complete. File saved to data/cleaned/DA/miso_da_combined_clean.csv\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/cleaned/DA/miso_da_combined_clean.csv'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"The file {file_path} does not exist. Running the processing code...\")\n",
    "    combine_cleaned_csvs()\n",
    "    # Make sure the directory exists before saving\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    print(f\"Processing complete. File saved to {file_path}\")\n",
    "else:\n",
    "    print(f\"The file {file_path} already exists. Skipping processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dce498f-51f1-4115-96fd-b045469c21a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.eia.gov/electricity/wholesalemarkets/data.php?rto=miso\n",
    "Load_Temp = pd.read_csv('data/dirty/Features/miso_load-temp_hr_2025.csv', skiprows = 3)\n",
    "Wind = pd.read_csv('data/dirty/Features/miso_gen_wnd_hr_2025.csv', skiprows = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f79e0c44-82de-441a-af03-d95903929e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Load_Temp = Load_Temp[['Local Timestamp Eastern Standard Time (Interval Beginning)', 'MISO Total Forecast Load (MW)', 'Indianapolis Temperature (Fahrenheit)']]\n",
    "Wind = Wind[['Local Timestamp Eastern Standard Time (Interval Beginning)', 'MISO Total Wind Generation (MW)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08884f08-489e-4348-9524-22fb848f5397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic merge on a single column\n",
    "Load_Temp_Wind = pd.merge(Load_Temp, Wind, on='Local Timestamp Eastern Standard Time (Interval Beginning)')\n",
    "\n",
    "# convert DataFrame date column to datetime format\n",
    "Load_Temp_Wind['Local Timestamp Eastern Standard Time (Interval Beginning)'] = pd.to_datetime(Load_Temp_Wind['Local Timestamp Eastern Standard Time (Interval Beginning)'])\n",
    "\n",
    "# Filter for April and May (months 4 and 5)\n",
    "Load_Temp_Wind = Load_Temp_Wind[Load_Temp_Wind['Local Timestamp Eastern Standard Time (Interval Beginning)'].dt.month.isin([4, 5])]\n",
    "\n",
    "#Rename columns\n",
    "Load_Temp_Wind.columns = ['datetime', 'Total Forecast Load', 'Temperature', 'Total Wind Gen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34cddcfe-76c4-402f-9134-b6875b9767e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file data/cleaned/Features/miso_load-temp-wind_hr_2025.csv already exists. Skipping processing.\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/cleaned/Features/miso_load-temp-wind_hr_2025.csv'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"The file {file_path} does not exist. Running the processing code...\")\n",
    "    Load_Temp_Wind.to_csv('data/cleaned/Features/miso_load-temp-wind_hr_2025.csv')\n",
    "    print(f\"Processing complete. File saved to {file_path}\")\n",
    "else:\n",
    "    print(f\"The file {file_path} already exists. Skipping processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02f740c1-ff56-4f41-87b9-dc0058258a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'data/dirty/DA_Constraints'  # Where your Excel files are\n",
    "file_paths = glob.glob(os.path.join(folder_path, '*.xls'))\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for path in file_paths:\n",
    "    filename = os.path.basename(path)\n",
    "    date_str = filename[:8]  # e.g., '20250401'\n",
    "    file_date = pd.to_datetime(date_str, format='%Y%m%d')\n",
    "\n",
    "    try:\n",
    "        # Skip the first 3 rows\n",
    "        df = pd.read_excel(path, skiprows=3)\n",
    "\n",
    "        # Make sure \"Hour of Occurrence\" column exists\n",
    "        if 'Hour of Occurrence' not in df.columns:\n",
    "            raise ValueError(f\"'Hour of Occurrence' not found in {filename}\")\n",
    "\n",
    "        # Create datetime column\n",
    "        df['datetime'] = df['Hour of Occurrence'].astype(int).apply(\n",
    "            lambda h: file_date + pd.Timedelta(hours=h)\n",
    "        )\n",
    "\n",
    "        all_dfs.append(df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {filename}: {e}\")\n",
    "\n",
    "# Combine all files into one DataFrame\n",
    "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Convert string to datetime if needed\n",
    "combined_df['datetime'] = pd.to_datetime(combined_df['datetime'])\n",
    "\n",
    "# Sort by datetime column (ascending order - oldest to newest)\n",
    "combined_df = combined_df.sort_values(by='datetime', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "717b1e4d-d646-4a84-ad2c-a5a32cc29fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df[['Constraint Name', 'datetime', 'Shadow Price']]\n",
    "\n",
    "# Suppose your constraint DataFrame is called constraints_df\n",
    "constraint_pivot = combined_df.pivot_table(\n",
    "    index='datetime',\n",
    "    columns='Constraint Name',\n",
    "    values='Shadow Price'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84d4c7b2-1d9f-494d-a7e1-ee950f9b5fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file data/cleaned/DA_Constraints/combined_DA_constraints.csv already exists. Skipping processing.\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/cleaned/DA_Constraints/combined_DA_constraints.csv'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"The file {file_path} does not exist. Running the processing code...\")\n",
    "    constraint_pivot.to_csv('data/cleaned/DA_Constraints/combined_DA_constraints.csv')    # Make sure the directory exists before saving\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    print(f\"Processing complete. File saved to {file_path}\")\n",
    "else:\n",
    "    print(f\"The file {file_path} already exists. Skipping processing.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0614de-8df5-4072-abbb-bc4865372a86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665db74b-39df-4ad3-bc94-bf5c49a57b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
